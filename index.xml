<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Aida’s home</title>
    <link>https://aida-yy.github.io/</link>
    <description>Recent content on Aida’s home</description>
    <image>
      <url>https://aida-yy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://aida-yy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 21 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://aida-yy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>HOW POWERFUL GNN</title>
      <link>https://aida-yy.github.io/posts/gnn_powerful_gin/</link>
      <pubDate>Sun, 21 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/gnn_powerful_gin/</guid>
      <description>HOW POWERFUL ARE GRAPH NEURAL NETWORKS?
本文是 Jure Leskovec 又一力作，首先对图神经网络的原理做了深入检出、提纲挈领的叙述，然后从原理方面介绍了如何发挥图神经网络的效用。
图神经网络可以分为三个阶段：
Aggregate：聚合邻居节点信息 $$ a^{(k)}{v} = AGGREGATE^{(k)}({h{\mu}^{(k−1)}:\mu\in N(v)}) $$
Combine：聚合邻居和当前节点 $$ h_{\mu}^{(k)} = COMBINE^{(k)}({h_{\mu}^{(k−1)},a^{(k)}_v}) $$
Readout：整合表示图中所有节点 $$ h_G = READOUT({h^{(K)}_v|v \in G}) $$
在GraphSAGE中，Aggregate和Combine过程如下，GCN同理
那么如何衡量图神经网络是否学到了良好的表示，这里提到了 Weisfeiler-Lehman test ，有兴趣可以下去研究。
文章的和核心出发点在于：对于子树结构相同且对应节点特征相同的的俩个节点，一个有效的GNN应该有能力映射两个节点到embedding空间中相同的位置，决不会将两个不同的节点映射到同一embedding空间位置。
Intuitively, a maximally powerful GNN maps two nodes to the same location only if they have identical subtree structures with identical features on the corresponding nodes
A maximally powerful GNN would never map two different neighborhoods</description>
    </item>
    
    <item>
      <title>ML&amp;二进制 paper list</title>
      <link>https://aida-yy.github.io/posts/ml%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%AE%BA%E6%96%87%E9%9B%86%E5%90%88/</link>
      <pubDate>Sat, 20 Aug 2022 20:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/ml%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%AE%BA%E6%96%87%E9%9B%86%E5%90%88/</guid>
      <description>survey title year A Survey of Machine Learning Methods and Challenges for Windows Malware Classification 2021 The rise of machine learning for detection and classification of malware: Research developments, trends and challenges 2019 Malware Detection and Cyber Security via Compression FUSING FEATURE ENGINEERING AND DEEP LEARNING: A CASE STUDY FOR MALWARE CLASSIFICATION 2022 malware detection paper title tag year An Investigation of Byte N-Gram Features for Malware Classification byte n-gram 2016 ⭐⭐⭐⭐ KiloGrams: Very Large N-Grams for Malware Classification byte n-gram；kilogram 2019 ⭐⭐⭐⭐ MalConv: Malware Detection by Eating a Whole EXE.</description>
    </item>
    
    <item>
      <title>docker快速上手</title>
      <link>https://aida-yy.github.io/posts/docker/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/docker/</guid>
      <description>Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口
安装 docker 设置仓库 $ sudo yum install -y yum-utils$ sudo yum-config-manager \--add-repo \https://download.docker.com/linux/centos/docker-ce.repo# 增加阿里源$ sudo yum-config-manager \--add-repo \http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装 Install the latest version of Docker Engine, containerd, and Docker Compose or go to the next step to install a specific version:
$ sudo yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin If prompted to accept the GPG key, verify that the fingerprint matches 060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35, and if so, accept it.</description>
    </item>
    
    <item>
      <title>并行化tf数据生成器</title>
      <link>https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/</guid>
      <description>在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项
使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。
本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。
tf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。
def func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理
A Tensor&amp;rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known.</description>
    </item>
    
    <item>
      <title>模型召回之DSSM</title>
      <link>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bdssm/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bdssm/</guid>
      <description>模型召回之DSSM 双塔模型
负样本构造：训练前构造或训练时批内构造
实现 model from transformers import AutoConfig,AutoTokenizer,TFAutoModel MODEL_NAME = &amp;#34;hfl/chinese-roberta-wwm-ext&amp;#34; tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) config = AutoConfig.from_pretrained(MODEL_NAME) # backbone = TFAutoModel.from_pretrained(MODEL_NAME) # tokenizer.save_pretrained(&amp;#39;model&amp;#39;) # config.save_pretrained(&amp;#39;model&amp;#39;) # backbone.save_pretrained(&amp;#39;model&amp;#39;) class baseModel(tf.keras.Model): def __init__(self,MODEL_NAME,finetune=False,pooler=&amp;#34;avg&amp;#34;): super().__init__() self.pooler = pooler self.backbone = TFAutoModel.from_pretrained(MODEL_NAME) if not finetune: self.backbone.trainable = False print(&amp;#34;bert close&amp;#34;) self.dense_layer = tf.keras.layers.Dense(128) def call(self,inputs): x = self.backbone(inputs) if self.pooler == &amp;#34;cls&amp;#34;: x = x[1] elif self.pooler == &amp;#34;avg&amp;#34;: x = tf.reduce_mean(x[0],axis=1) elif self.</description>
    </item>
    
    <item>
      <title>模型召回之SimCSE</title>
      <link>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bsimcse/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bsimcse/</guid>
      <description>模型召回之SimCSE
[TOC]
dataset unsuper import numpy as np import math class UnsuperviseData(tf.keras.utils.Sequence): def __init__(self, x_set, batch_size): self.x = x_set self.batch_size = batch_size def __len__(self): return math.ceil(len(self.x) / self.batch_size) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_x = batch_x + batch_x bx = np.array([batch_x[i::self.batch_size] for i in range(self.batch_size)]).flatten().tolist() return self._tokenizer(bx) def _tokenizer(self,x): return tokenizer(x, max_length=50, padding=True,truncation=True,return_tensors=&amp;#34;tf&amp;#34;) super class SuperviseData(tf.keras.utils.Sequence): def __init__(self, query_set, doc_set, corpus, batch_size): self.querys = query_set self.</description>
    </item>
    
    <item>
      <title>策略分析-macd</title>
      <link>https://aida-yy.github.io/posts/%E7%AD%96%E7%95%A5%E5%88%86%E6%9E%90-macd/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E7%AD%96%E7%95%A5%E5%88%86%E6%9E%90-macd/</guid>
      <description>MACD（Moving Average Convergence / Divergence）称为指数平滑移动平均线，是从双指数移动平均线发展而来的，由快的指数移动平均线减去慢的指数移动平均线得到两者的差值DIF，再用2×（DIF-DIF的9日加权移动均线DEA）得到MACD柱。MACD的意义和双移动平均线基本相同，即由快、慢均线的离散、聚合表征当前的多空状态和股价可能的发展变化趋势，但比简单移动均线系统更敏感，更容易辨识交易信号。当MACD从负数转向正数，是买的信号。当MACD从正数转向负数，是卖的信号。当MACD以大角度变化，表示快的移动平均线和慢的移动平均线的差距非常迅速的拉开，代表了一个市场大趋势的转变。
MACD公式如下： DIFF=EMA(12)－EMA(26) #指标的快速线 DEA=EMA(DIFF,9) #指标的慢速线 MACD=2×(DIFF－DEA) #MACD红绿柱子 止损： 止损是控制风险的必要手段。在证券市场中指交易仓位获允许承受的最大损失，一旦达到损失上限，交易员必须平仓或减仓以阻止亏损扩大。止损方法常用的为两类：固定止损和移动止损。
固定止损：顾名思义，买入后设定一个固定的回撤止损比例，只要所交易的标的跟买入价相比资产回撤达到了设置的止损比例，则平仓。 移动止损：又称“追踪止损”，即追随买入证券最新价格设置的止损。最简单的移动止损策略是：监测当前状态的买入标的的价格相对与前一交易日标的价格的回撤幅度进行止损，当回撤达到预设比例后平仓。 本研究的策略是： 买入：MACD低位金叉买入。低位的定义是，DIF，DEA都小于0。金叉，指DIF线从下上穿DEA线。 卖出：MACD死叉卖出。死叉，指DIF线从上下穿DEA线 止损：固定止损卖出，移动止损卖出。 策略参数： MACD参数，快速EMA周期：12，慢速EMA周期：26，DIFF平滑周期：9。 止损参数，固定止损：0.93（回撤7%），移动止损：0.95（回撤5%)。 回测标的：招商银行[600036.SH] 回测周期：20150101-20180101 </description>
    </item>
    
    
    
    <item>
      <title>书单</title>
      <link>https://aida-yy.github.io/booklist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/booklist/</guid>
      <description>booklist</description>
    </item>
    
  </channel>
</rss>
